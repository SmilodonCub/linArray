{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bedd74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Array Viewer\n",
    "## a Streamlit App to Visualize Experimental Neuroscience Data Deployed with Heroku\n",
    "### Final Project Report for Data608\n",
    "<br>\n",
    "\n",
    "Bonnie Cooper  \n",
    "bcooper@sunyopt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc5763",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a Short 2-3 Paragraph Visualization Write-up\n",
    "\n",
    "\n",
    "For a Final Project, I have developed and deployed a web-based app to visualize experimental neuroscience data collected in the [McPeek lab at SUNY College of Optometry](https://sunyopt.edu/labs/McPeek/McPeek_Lab/Home.html) where I am a post-doctoral researcher. Our lab investigates the neural guidance of voluntary eye-movements (saccades) and some aspects of the allocation of attention. We do this by conducting behavioral experiments that collect eye-movement data while simultaneously recording neural activity from a few regions of the brain that are known to be key nodes in the circuitry that controls the execution of saccades. This app visualizes data collected from a few of the experiments we have been running lately. This app is intended for lab personnel (not for a broad audience) as a means to assess the quality of the data after a series of experiments. This is important, because catching acquisition issues early (e.g. faulty event marker assignments or noisy eye-tracking) can save a lot of time and frustration.  \n",
    "\n",
    "The data is collected from a small acquisition network and has two main sources of origin: behavioral eye-tracking data from an [Eyelink1000 from SR Research](https://www.sr-research.com/eyelink-1000-plus/) and neural data collected from [16 channel linear arrays](https://plexon.com/products/plexon-u-probe/) on a [Plexon OmniPlex system](https://plexon.com/data-acquisition-omniplex/). The data undergoes some preprocessing using proprietary Plexon software. We then use some custom Matlab code to shape the data into a form which can be imported into the Python environment.\n",
    "\n",
    "To develop this app, a custom [module of Python functions](https://github.com/SmilodonCub/linArray/blob/master/linArray_app/alignment_functions.py) was developed to process and shape the data. This involves some simple computations to adjust the timing of the neural and behavioral data to be synchronized according to timestamp eventmarkers that define key points in an experimental trial. From this, a series of visualizations were developed in Jupyter notebooks ([notebook used to develop functions](https://github.com/SmilodonCub/linArray/blob/master/linArray_notebook.ipynb), [notebook used to finetune for specific experiments](https://github.com/SmilodonCub/linArray/blob/master/linArray_data_organization.ipynb)) to show some summary figures that visualize the behavioral and neural data aligned to (1) the appearance of visual targets and (2) the behavioral response of the subject. The [Linear Array Viewer app](https://github.com/SmilodonCub/linArray/tree/master/linArray_app) was built using Streamlit and was deployed to [Heroku](https://murmuring-meadow-80918.herokuapp.com/).\n",
    "\n",
    "\n",
    "If you are curious and would like to read more there is a more detailed write-up that follows (with more context and background)...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff29b30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* Background\n",
    "* the Data\n",
    "* the Visualization\n",
    "* the App\n",
    "* Observations\n",
    "* Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2a223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "This report describes an app developed to aid in the evaluation and exploration of experimental neuroscience data through a series of visualizations. The experiments range in complexity but have a central theme: visual search experiments that manipulate the allocation of attention to explore how neurons in the superior colliculus represents various aspects of the decision process that contributes to the guidance of voluntary eye-movements. We target the superior colliculus because it is a philogenetically well-preserved region of the brain known to be a crucial node in the neural circuitry that controls the execution of voluntary eye-movements (saccades). Saccades are of interest to us because they play an important role in the allocation of attention in the primate visual system. Additionally, saccades serve as a proxy to study neural representations of the decision process that can be applied to other sensory-motor behaviors which is of importance to brain-computer interface research. The data was collected across several experiments that simultaniously record behavioral responses in the form of eye-movement data combined with physiological data recorded from laminar multielectrode arrays lowered into the superior colliculus in the midbrain. The app presented here produces exploratory visualizations of lightly preprocess data to aid in the evaluation of experimental data quality.  \n",
    "\n",
    "The superior colliculus is a phylogenetically ancient structure situated on the roof of the midbrain. It is remarkably well preserved: if a vertebrate species on the planet Earth moves it's eyes, it does so by circiutry routed through the superior colliculus (midbrain tectum in non-mammals). This property makes the colliculus a remarkable structure to study the neural representation of sensory-motor transduction. We study these properties by lowing electrodes into the brain to record neural activity while subjects perform visual search behavioral tasks. We are able to records simultaneous neural and behavioral activity while the subjects weights sensory evidence in the form a visual simuli and makes a decision based on this sensory evidence to guide a propor motor response in the form of a saccade to a task relevant stimulus.  \n",
    "\n",
    "Neurons in the primate superior colliculus have interesting properties. Cell activity can represent visual responses to stimuli in an area of the visual field, the Visual Response Field, or the preparation to execute an eye movement to a given vector (direction and amplitude) in visual space referred to as the cell's Motor Response Field. Some cells, particularly those in the superficial layer of the colliculus, give a purely visual response. Cells that lay deeper may have a purely motor response. However, many cells have a combined response that represents both visual and motor components that contribute to the decision to execute an eye-movement to a particular region of visual space. The superior colliculus is a remarkably ordered region of the brain: visual and motor response fields are ordered across the colliculus topography in remarkable correspondence to visual space. \n",
    "\n",
    "For each experiment, a subject is placed in a dark room in front of a monitor equiped with an Eyelink1000 eye-tracker from SR Research and neural data is acquired from 16 channel u- or v-probes from Plexon. These particular arrays are liminar (linear) and are situated in the brain such that the channels record across the depth of superior colliculus. The app currently supports visualizations from three different experimental paradigms. Personally, I describe these experiments as the most boring video games you'll ever have to play with your eye movements. However, a more formal description is given as follows:  \n",
    "\n",
    "* **Delayed Saccade Task:**  \n",
    "    - **procedure** a subject must hold fixation on a small icon at the center of the screen. after a variable length of time, a target appears in the periphery. the target is situated such that it falls within the response field of cells in the neighborhood of the electrode. Delay period: the subject must hold fixation at the center until the fixation icon vanishes (variable length of time). only then can the subject look at the target and collect a reward.\n",
    "    - **purpose** the variable length delay period is intended to tease out the visual annd motor components of the neural response for those intermediate cells that are neither fully visual or pure motor. It is these intermediate 'visuomotor' cells that are thought to play an important role in the representation of decision making in superior colliculus. This task is not so much an experiment (this property is well known and has been studied for decades) rather, the delayed saccade task is used to classify cells as Visual, Motor or Visuomotor cells.\n",
    "    \n",
    "* **Delayed Saccade 180 Task** \n",
    "    - **procedure** similar to delayed saccade, however, the target either appears in the reponse field of the cell or at a location in a 180 rotation position.\n",
    "    - **purpose** we are interested in observing suppression signals in the cells response properties when a saccade is being planned in the opposite direction\n",
    "    \n",
    "* **Categorical Search**\n",
    "    - **procedure** subjects were trained to perform a visual search for target images (teddy bears and butterflies) in a field of distractor images (from numerous random categories). On half of the trials, a target image is present and the subject is rewarded for finding the target and holding fixation for 500ms. On the other half of the trials, there is no target image present. The subject must indicate the absense of a target by fixating an 'opt-out' target located outside the search array.\n",
    "    - **purpose** we have developed a model of attention in the superior colliculus. we are interested in trials where a distractor image is located in the neurons response field. Is feature similarity of distractor images predictive of neural activity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673f371",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the Data\n",
    "\n",
    "* **the System** a small network of acquisition machines:\n",
    "    * behavior machine to control the experiment and send strobed bits to mark events in trials\n",
    "    * SR EyeLink machine to detect/record eye-movement data and send to the behavior machine\n",
    "    * Plexon neural acquisition receives behavior strobes to timestamp neural data\n",
    "* **the Neural Data** 16 channel laminar arrays using Plexon Omniplex acquisition system. The resulting datafile is preprocessed using Plexon's proprietary software to isolate the neural activity of individual neurons (some combination of PCA and clustering). The another proprietary software package (NeuroExplorer) is used to parse the data into individual trials and export information to MATLAB as a .mat structure. \n",
    "* **the Behavioral Data** the behavior machine runs MATLAB and uses a toolbox developed by NIMH called MonkeyLogic. I write custom MATLAB code for [MonkeyLogic](https://monkeylogic.nimh.nih.gov/) to run my experiments and the result is a behavioral '.bhv' file which holds the analog eye-tracking data and experimental event timestamps (used to align data across files).\n",
    "* **the Problem** Previously, I have written custom software (predominantly MATLAB) to bridge the processing steps. What is new for this app: I have developed Python code to translate the MATLAB stuctures into to pandas dataframes. Furthermore, both the neural and eye-tracking data need to be aligned on specific behavioral event markers in order to interrelate the two data sets. \n",
    "    - why is this difficult? \n",
    "        - the MATLAB structures get imported to Python as deeply nested dictionary structure that are very difficult to parse\n",
    "        - the time series of the Plexon and MonkeyLogic files do not correspond directly: I am reliant on correctly interpreting the eventmarker timestamps in each file. I wish this was straightforward, but often, it is not!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaea603",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the Visualizations\n",
    "\n",
    "The Linear Array Viewer App is intended for lab personnel to get an overview of the data to evaluate whether the experiment is fit for downstream analysis. There is a panel of visualizations for each of the three experiments:  \n",
    "\n",
    "1. **Delayed Saccade** This is a diagnostic task to assess the strength of a cells visual and motor responses.\n",
    "    * Left Column: We are interested in viewing the visual component by aligning the data on the appearance of the target in the periphery\n",
    "    * Right Column: We are interested in viewing the motor components by aligning the data on the initiation of a saccade\n",
    "    * Top Row: Neural Responses as spiking activity binned as a histogram and smoothed by applying a gaussian filter\n",
    "    * Bottom Row: EyeTraces: X and Y components of the eye-position. Important to point out that the x-axis (time) is aligned to correspond for both neural and behavioral data.\n",
    "    * **What to Look For?**\n",
    "        - are there a lot of spikes for a given channel? which channels have the most spikes\n",
    "        - does it look like the saccade initiations are being accurated calculated? (do the eyetraces in the saccade initiation plot have strong overlap or are some offset?)\n",
    "        - are the eye-traces noisy? (do you see sharp pulses? these are likely from blinks (there is an old rule: noisy eyedata == noisy neural data))\n",
    "        - do you see a strong visual peak in the Stimulus Onset neural alignments?\n",
    "        - do you see evidence of a motor peak in the Saccade Initiation neural alignments?\n",
    "        \n",
    "    \n",
    "2. **Delayed Saccade 180 Task** \n",
    "    * Same arrangement as Delayed Saccade: left and right columns give Stimulus Onset and Saccade Initiation alignments respectively with neural data in the top row and eye-trace data in the bottom.\n",
    "    * What's New: the data has been shaped to show targets in/out of the response field as red/green data series respectively\n",
    "    * **What to look for?**\n",
    "        - (same observations as with delayed saccade)\n",
    "        - Is there a clear difference in activity when the target is in the cell's response field vs out of the response field\n",
    "    \n",
    "3. **Categorical Search** \n",
    "    * Left Column: Familiar Visualizations of Stimulus Onset neural (top) and eye-trace (bottom) figures. There is no variable delay for this task, so we do not show a saccade onset alignment (without a delay saccade activity overlaps visual activity)\n",
    "    * Right Panel: the previously described eye-trace figures show the x and y amplitudes as a function of time. This panel renders the same analog eye-data as measured in 2D visual space. The result can be thought of as heat map indicating regions of the visual display that the subject attended.\n",
    "    * **What to Look For?**\n",
    "        - evaluate the Stimulus Onset neural and eyetrace plots as described above\n",
    "        - Eye-location heat map: Do you see any large artifacts? Do most of the eye-movements fall within the array and move inbetween target positions ....are there signs of misbehaving?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4958f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the App\n",
    "\n",
    "Without further delay: [The Linear Array Viewer App]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf62b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Viewing Options\n",
    "\n",
    "* **Track** pick between one of three experimental recording sessions.\n",
    "    - **Furture Implementation** - will have routines to upload local data to generate visualizations\n",
    "* **Task** each Track holds data for multiple experimental tasks. Once a 'Track' is selected, the dropdown 'Task' menu will list task options available for a given Track\n",
    "* **Channel** there are a maximum of 16 channels that can record simultaneously. Many recordings do not have all 16 channels associated, because channels may be dropped in preprocessing if they were determined to be empty of spiking activity. Once a 'Track' is selected, the Channel dropdown menu with list available channels.\n",
    "    - **Pro-tip**: channels with a higher number penetrate deeper in the colliculus and are more likely to have higher activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce623c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Observations\n",
    "\n",
    "* The three tracks uploaded with the app were recorded on the same day ('T35'==35th recording) in succession 't1' $\\rightarrow$ 't2' $\\rightarrow$ 't3'. In between each track, the linear array was advanced ~0.5mm deeper into the brain. We can see evidence of this by seeing which channels are active for each track. For instance, 't1' has only the bottom 4-5 channels active whereas comparatively more channels in 't3' are active.\n",
    "* We see many Visual peaks in the Stimulus Onset alignments, but not much evidence at all of distinct Motor peaks in the Saccade Initiation alignments; this is likely because the array did not go deep enough in the superior colliculus to 'find' motor cells.\n",
    "* There are only two examples of the 'Delayed Saccade 180' Task. For one recording, there is a pronounced difference between the high activity levels when the target is in the saccade vs out - this is the expected result. However, for the other recording there is little difference between activity levels. Puzzling! Is our electrode in the right place?....could be a site close to the superior colliculus but not quite there.\n",
    "* Viewing the two 'Categorical Search' files: the behavior looks clean for one file, however another has large artifacts (likely blinks) in the eye-traces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd2f35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "And there you have it!  \n",
    "This small applet allows lab personnel to visualize experimental data to assess the quality of the recordings and make some classification judgements on the response properties of neurons on given channels of a recording track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bef4f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
